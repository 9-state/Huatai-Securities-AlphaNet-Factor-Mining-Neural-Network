{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b349dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from audtorch.metrics.functional import pearsonr\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2ff734",
   "metadata": {},
   "source": [
    "# 自定义卷积核(特征提取)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ee78149",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "过去d天X值构成的时序数列和Y值构成的时序数列的相关系数\n",
    "'''\n",
    "\n",
    "class ts_corr(nn.Module):\n",
    "    def __init__(self, d=10, stride=10):\n",
    "        super(ts_corr, self).__init__()\n",
    "        self.d = d\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, X): #X:3D\n",
    "        B, n, T = X.shape # 批量大小，特征数量，时间窗口长度\n",
    "        \n",
    "        w = (T - self.d) // self.stride + 1  # 窗口数量,例如T=10，d=3，stride=2时，w=4\n",
    "        h = n * (n - 1) // 2  # 特征对的数量 C(n, 2)\n",
    "\n",
    "        # 使用 unfold 提取滑动窗口 [形状: (B, n, w, d)]\n",
    "        unfolded_X = X.unfold(2, self.d, self.stride)\n",
    "        \n",
    "        #生成C(n,2)组合数\n",
    "        #例如当n=3时，rows = tensor([0,0,1]), cols = tensor([1,2,2])\n",
    "        rows, cols = torch.triu_indices(n, n, offset=1)\n",
    "\n",
    "        # 提取特征对数据得到x和y [形状: (B, h, w, d)]，分别对应batch维度，特征维度，窗口维度，时间维度\n",
    "        #x为([[:,0,:,:],[:,0,:,:],[:,1,:,:])\n",
    "        #y为([[:,1,:,:],[:,2,:,:],[:,2,:,:])\n",
    "        \n",
    "        x = unfolded_X[:, rows, :, :]\n",
    "        y = unfolded_X[:, cols, :, :]\n",
    "        \n",
    "        x_mean = torch.mean(x, dim=3, keepdim=True) #keepdim维持原本维度不变,在维度3做mean\n",
    "        y_mean = torch.mean(y, dim=3, keepdim=True)\n",
    "        \n",
    "        cov = ((x-x_mean)*(y-y_mean)).sum(dim=3) #(B, h, w)\n",
    "        corr = cov / (torch.std(x, dim=3) * torch.std(y, dim=3)+ 1e-8) #分母添加极小值防止除零错误\n",
    "\n",
    "        return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "101be042",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "过去d天X值构成的时序数列和Y值构成的时序数列的协方差\n",
    "'''\n",
    "\n",
    "class ts_cov(nn.Module):\n",
    "    def __init__(self, d=10, stride=10):\n",
    "        super(ts_cov, self).__init__()\n",
    "        self.d = d\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, X):\n",
    "        B, n, T = X.shape \n",
    "        \n",
    "        w = (T - self.d) // self.stride + 1  \n",
    "        h = n * (n - 1) // 2  \n",
    "\n",
    "        unfolded_X = X.unfold(2, self.d, self.stride)\n",
    "        \n",
    "        rows, cols = torch.triu_indices(n, n, offset=1)\n",
    "\n",
    "        x = unfolded_X[:, rows, :, :]\n",
    "        y = unfolded_X[:, cols, :, :]\n",
    "        \n",
    "        x_mean = torch.mean(x, dim=3, keepdim=True)\n",
    "        y_mean = torch.mean(y, dim=3, keepdim=True)\n",
    "        \n",
    "        cov = ((x-x_mean)*(y-y_mean)).sum(dim=3) \n",
    "\n",
    "        return cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34ba47b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "过去d天X值构成的时序数列的标准差\n",
    "'''\n",
    "\n",
    "class ts_stddev(nn.Module):\n",
    "    \n",
    "    def __init__(self, d = 10, stride = 10):\n",
    "        super(ts_stddev,self).__init__()\n",
    "        self.d = d\n",
    "        self.stride = stride\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #input:(B,n,T)，在T维度用unfold展开窗口，变为(B,n,w,d),w为窗口数量会自动计算\n",
    "        unfolded_X = X.unfold(2, self.d, self.stride)\n",
    "        #在每个窗口，即d维度上进行std计算\n",
    "        std = torch.std(unfolded_X, dim=3) #输出形状为(B,n,w)\n",
    "        \n",
    "        return std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb5c0d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "过去d天X值构成的时序数列的平均值除以标准差\n",
    "'''\n",
    "\n",
    "class ts_zscore(nn.Module):\n",
    "    \n",
    "    def __init__(self, d = 10, stride = 10):\n",
    "        super(ts_zscore,self).__init__()\n",
    "        self.d = d\n",
    "        self.stride = stride\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        unfolded_X = X.unfold(2, self.d, self.stride)\n",
    "        \n",
    "        mean = torch.mean(unfolded_X, dim=3)\n",
    "        std = torch.std(unfolded_X, dim=3)\n",
    "        zscore = mean / (std + 1e-8)\n",
    "        \n",
    "        return zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8df1a998",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "研报原话为：\n",
    "(X - delay(X, d))/delay(X, d)-1, delay(X, d)为 X 在 d 天前的取值\n",
    "这里可能有误，return为“收益率“，应该是误加了-1\n",
    "为了保持代码一致性，这里计算的是(X - delay(X, d-1))/delay(X, d-1),  delay(X, d-1)为 X 在 d-1 天前的取值\n",
    "在构造卷积核的逻辑上是相似的\n",
    "'''\n",
    "\n",
    "class ts_return(nn.Module):\n",
    "    \n",
    "    def __init__(self, d = 10, stride = 10):\n",
    "        super(ts_return,self).__init__()\n",
    "        self.d = d\n",
    "        self.stride = stride\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        unfolded_X = X.unfold(2, self.d, self.stride)\n",
    "        return1 = unfolded_X[:,:,:,-1] /(unfolded_X[:,:,:,0] + 1e-8) - 1\n",
    "        \n",
    "        return return1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0496fadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "过去d天X值构成的时序数列的加权平均值，权数为d, d – 1, …, 1(权数之和应为1，需进行归一化处理）\n",
    "其中离现在越近的日子权数越大。 \n",
    "'''\n",
    "\n",
    "class ts_decaylinear(nn.Module):\n",
    "    \n",
    "    def __init__(self, d = 10, stride = 10):\n",
    "        super(ts_decaylinear,self).__init__()\n",
    "        self.d = d\n",
    "        self.stride = stride\n",
    "        #如下设计的权重系数满足离现在越近的日子权重越大\n",
    "        weights = torch.arange(d, 0, -1, dtype = torch.float32) \n",
    "        weights = weights / weights.sum()\n",
    "        #注册权重，不用在前向传播函数中重复计算\n",
    "        #注册了一个形状为(d,)的一维张量，存放权重系数，以便在forward函数中使用\n",
    "        self.register_buffer('weights', weights) \n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        unfolded_X = X.unfold(2, self.d, self.stride)\n",
    "        #view将一维张量广播为4D张量，并在时间维度上，将weights与unfoled_X相乘\n",
    "        decaylinear = torch.sum(unfolded_X * self.weights.view(1,1,1,-1), dim=-1)\n",
    "        \n",
    "        return decaylinear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2a4748",
   "metadata": {},
   "source": [
    "# 神经网络结构设计 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5097af",
   "metadata": {},
   "source": [
    "原始路径(RawPath)：特征提取层→BN\n",
    "\n",
    "池化路径(PoolPath)：特征提取层→池化层→BN\n",
    "\n",
    "展平→全连接层→预测目标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53c5434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "原始路径：特征提取层+BN\n",
    "'''\n",
    "class RawPath(nn.Module):\n",
    "    def __init__(self, extractor, bn_dim): #传入参数：卷积核，特征维度\n",
    "        super().__init__()\n",
    "        self.extractor = extractor\n",
    "        self.bn_dim = bn_dim\n",
    "        \n",
    "    def forward(self, X):\n",
    "        x = self.extractor(X) #extract\n",
    "        x = nn.BatchNorm1d(self.bn_dim)(x) #BN\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e1a250e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "池化路径：特征提取层+池化层+BN\n",
    "'''\n",
    "class PoolPath(nn.Module): #传入参数：卷积核，特征维度\n",
    "    def __init__(self, extractor, bn_dim, d_pool=3, s_pool=3):\n",
    "        super().__init__()\n",
    "        self.extractor = extractor\n",
    "        self.bn_dim = bn_dim\n",
    "        self.avg_pool = nn.AvgPool1d(d_pool, s_pool)\n",
    "        self.max_pool = nn.MaxPool1d(d_pool, s_pool)\n",
    "\n",
    "    def forward(self, X):\n",
    "        x = self.extractor(X)\n",
    "        \n",
    "        x_avg = self.avg_pool(x)\n",
    "        x_avg = nn.BatchNorm1d(self.bn_dim)(x_avg)\n",
    "        \n",
    "        x_max = self.max_pool(x)\n",
    "        x_max = nn.BatchNorm1d(self.bn_dim)(x_max)\n",
    "        \n",
    "        x_min = -self.max_pool(-x) #手动取反实现min_pool\n",
    "        x_min = nn.BatchNorm1d(self.bn_dim)(x_min)\n",
    "        \n",
    "        return torch.cat([x_avg, x_max, x_min], dim = 1) #在特征维度进行拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ff467db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaNet(nn.Module):\n",
    "\n",
    "    def __init__(self, d=10, stride=10, d_pool=3, s_pool=3, n=9, T=30): #池化层窗口d=3，步长stride=3\n",
    "        super(AlphaNet, self).__init__()\n",
    "        \n",
    "        self.d = d \n",
    "        self.stride = stride \n",
    "        h = n * (n - 1) // 2 #手动计算cov和corr特征提取后的特征维度大小\n",
    "        w = (T - d) // stride + 1 #手动计算特征提取层窗口数\n",
    "        w_pool = (w - d_pool) // s_pool + 1 #手动计算池化层窗口数\n",
    "        \n",
    "        #特征提取层列表，共7个\n",
    "        self.extractors = nn.ModuleList([\n",
    "            ts_corr(d,stride),\n",
    "            ts_cov(d,stride),\n",
    "            ts_stddev(d,stride),\n",
    "            ts_zscore(d,stride),\n",
    "            ts_return(d,stride),\n",
    "            ts_decaylinear(d,stride),\n",
    "            nn.AvgPool1d(d,stride) #原研报中的ts_mean\n",
    "        ])\n",
    "        \n",
    "\n",
    "        # 初始化双路径\n",
    "        self.raw_paths = nn.ModuleList()\n",
    "        self.pool_paths = nn.ModuleList()\n",
    "        \n",
    "        # 前两个特征提取器使用h维BN\n",
    "        for i in range(2):\n",
    "            self.raw_paths.append(RawPath(self.extractors[i], h))\n",
    "            self.pool_paths.append(PoolPath(self.extractors[i], h))\n",
    "        \n",
    "        # 后五个特征提取器使用n维BN\n",
    "        for i in range(2, 7):\n",
    "            self.raw_paths.append(RawPath(self.extractors[i], n))\n",
    "            self.pool_paths.append(PoolPath(self.extractors[i], n))\n",
    "\n",
    "        raw_dim = (2*h + 5*n)*w #计算初始路径展平后的维度\n",
    "        pooled_dim = (h*2*3 + n*5*3)*w_pool #计算池化路径展平后的维度\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(raw_dim + pooled_dim, 30),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(30, 1)\n",
    "        )\n",
    "\n",
    "        # 初始化线性层和输出层\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                #nn.init.trunc_normal_(m.weight)\n",
    "                #nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.normal_(m.bias, std=1e-6)\n",
    "                #nn.init.zeros_(m.bias)\n",
    "            \n",
    "            #if m is self.head[-1]:\n",
    "                #nn.init.normal_(m.weight, std=0.01)\n",
    "            \n",
    "            #elif isinstance(m, nn.BatchNorm1d):\n",
    "                #nn.init.ones_(m.weight)\n",
    "                #nn.init.normal_(m.bias, std=1e-4)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        raw_features = [path(X).flatten(1) for path in self.raw_paths] #原始路径得到的张量进行展平\n",
    "        pool_features = [path(X).flatten(1) for path in self.pool_paths] #池化路径得到的张量进行展平\n",
    "        all_features = torch.cat(raw_features + pool_features, dim=1) \n",
    "        \n",
    "        return self.head(all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab419bff",
   "metadata": {},
   "source": [
    "# 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95e9ec6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X:  (930008, 9, 30)\n",
      "Shape of Y:  (930008,)\n"
     ]
    }
   ],
   "source": [
    "X = np.load('X_fe.npy')\n",
    "Y = np.load('Y_fe.npy')\n",
    "dates = np.load('Y_dates.npy')\n",
    "\n",
    "print('Shape of X: ', X.shape)\n",
    "print('Shape of Y: ', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74e91eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataset(Dataset):\n",
    "    '''\n",
    "    自定义数据集，将原始数据从 numpy arrays 转换成 float 格式的 tensors\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        super(myDataset, self).__init__()\n",
    "        self.X = torch.tensor(X).float()\n",
    "        self.y = torch.tensor(y).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24c438a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphanet = AlphaNet(d=10, stride=10, n=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "865323c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.5642],\n",
       "        [-2.7500],\n",
       "        [-2.2397],\n",
       "        [ 0.6137],\n",
       "        [-1.8670]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphanet(torch.tensor(X[:5]).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85329938",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dates = np.array([datetime.strptime(str(date), '%Y-%m-%d').date() for date in dates])\n",
    "unique_dates = sorted(np.unique(target_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b28cc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#用于得到不同的轮次,确保每个轮次为1626天\n",
    "\n",
    "start_dates = []\n",
    "starts, ends = [], []\n",
    "\n",
    "i, start, end = 0, 0, 0\n",
    "while i + 1500 + 126 <= len(unique_dates):\n",
    "    start_dates.append(i)\n",
    "    start = sum(target_dates < unique_dates[i])\n",
    "    starts.append(start)\n",
    "    end = sum(target_dates < unique_dates[i+1500+126])\n",
    "    ends.append(end)\n",
    "    i += 126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a9fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 设置随机种子，保证训练结果一致\n",
    "torch.manual_seed(42)\n",
    "#torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# 设置训练参数：学习率、训练迭代次数、批量大小\n",
    "lr = 0.0001\n",
    "n_epoch = 50\n",
    "batch_size = 1000\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model_name = 'alphanet'\n",
    "net = AlphaNet(d=10, stride=10, n=9)\n",
    "\n",
    "# 初始化损失函数和优化器\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "# 初始化一个字典，用来储存模型训练期间的表现\n",
    "results = {}\n",
    "results['round'] = []\n",
    "results['train'] = []\n",
    "results['valid'] = []\n",
    "results['test'] = []\n",
    "\n",
    "# 维护 cnt 变量，记录当前是第几个训练轮次\n",
    "cnt = 0\n",
    "\n",
    "# 滚动窗口\n",
    "for start, end in zip(starts, ends):\n",
    "    \n",
    "    n = end - start\n",
    "    train_size = int(n * 0.7)\n",
    "    valid_size = int(n * 0.2)\n",
    "\n",
    "    train_end = start + train_size\n",
    "    valid_end = train_end + valid_size\n",
    " \n",
    "    train_set = myDataset(X[start:train_end], Y[start:train_end])\n",
    "    valid_set = myDataset(X[train_end:valid_end], Y[train_end:valid_end])\n",
    "    test_set = myDataset(X[valid_end:end], Y[valid_end:end])\n",
    "    \n",
    "    #train_size = int(n * 0.7)\n",
    "    #test_size = int(n * 0.2)\n",
    "\n",
    "    #train_end = start + train_size\n",
    "    #valid_end = train_end + valid_size\n",
    " \n",
    "    #train_set = myDataset(X[start:train_end], Y[start:train_end])\n",
    "    #valid_set = myDataset(X[train_end:valid_end], Y[train_end:valid_end])\n",
    "    #test_set = myDataset(X[valid_end:end], Y[valid_end:end])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 创建loader\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=False)\n",
    "    valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # 当前训练轮次的模型储存地址\n",
    "    model_path = 'Models/' + model_name + '_' + str(cnt) + '.pt'\n",
    "    \n",
    "    count = 0\n",
    "    train_loss_lst, valid_loss_lst = [], []\n",
    "    best_valid_loss = float('inf')\n",
    "    patience = 5\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        \n",
    "        # 训练\n",
    "        net.train()\n",
    "        train_loss = 0\n",
    "        for x, y in tqdm(train_loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            preds = net(x)\n",
    "            loss = criterion(preds, y.unsqueeze(dim=1))\n",
    "            train_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_loss /= len(train_loader.dataset.X)\n",
    "        \n",
    "        # 验证\n",
    "        net.eval()\n",
    "        valid_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in tqdm(valid_loader):\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                preds = net(x)\n",
    "                loss = criterion(preds, y.unsqueeze(dim=1))\n",
    "                valid_loss += loss.item()  \n",
    "        valid_loss /= len(train_loader.dataset.X)\n",
    "        \n",
    "        # 监测训练效果\n",
    "        print(\"Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}\".format(epoch+1, train_loss, valid_loss))\n",
    "        \n",
    "        # 记录训练效果\n",
    "        train_loss_lst.append(train_loss)\n",
    "        valid_loss_lst.append(valid_loss)\n",
    "        \n",
    "        # 更新本地模型\n",
    "        if valid_loss < best_valid_loss - 1e-4:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(net.state_dict(), model_path)\n",
    "            print(\"Saved model with validation loss of {:.4f}\".format(best_valid_loss)) \n",
    "            count = 0\n",
    "        else:\n",
    "            count += 1\n",
    "            \n",
    "        # 早停：若累计有patience次迭代，模型都没有进步，停止本轮训练\n",
    "        if count >= patience:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    # 记录当前训练轮次的指标变动，并更新本地储存结果\n",
    "    results['round'].append(str(cnt))          \n",
    "    results['train'].append(train_loss_lst)   \n",
    "    results['valid'].append(valid_loss_lst)   \n",
    "    with open('train_results.pickle', 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    \n",
    "    # 下一轮\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658dbfb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2578723b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cbd25c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a13e94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
